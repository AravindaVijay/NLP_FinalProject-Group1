{
    "clean_data": "Sr HadoopSpark Developer Sr HadoopSpark span lDeveloperspan Sr HadoopSpark Developer Sprint Indianapolis IN Extensive IT experience of over 9 years in Analysis Design Development Implementation Maintenance and Support with experience in developing strategic methods for deploying Big Data technologies to efficiently solve Big Data processing requirement Around 5 years of experience on BIG DATA using HADOOP framework and related technologies such as HDFS Map Reduce HIVE PIG YARN APACHE SPARK FLUME KAFKA OOZIE SQOOP ZOOKEEPER and NoSQL Databases like HBase Cassandra Worked extensively on Hadoop Gen1 and Gen2 and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Resource Manager YARN Experience in working with Amazon EMR Cloudera CDH4CDH5 and Horton Works Hadoop Distributions Capable of processing large sets of structured semistructured and unstructured data and supporting systems application architecture Extensively used Apache Sqoop for efficiently importing and exporting data from HDFS to Relational Database Systems and from RDBMS to HDFS Worked on data load from various sources ie Oracle MySQL DB2 MS SQL Server Cassandra Hadoop using Sqoop and Python Script Experience in developing data pipeline using Sqoop and Flume to extract the data from weblogs and store in HDFS Experience in managing and reviewing Hadoop Log files using FLUME and Kafka and also developed the Pig UDFs and Hive UDFs to preprocess the data for analysis Worked on Impala for Massive parallel processing of Hive queries Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Efficient in working with Hive data warehouse tool creating tables data distributing by implementing Partitioning and Bucketing strategy writing and optimizing the HiveQL queries Experience in ingestion storage querying processing and analysis of Big Data with hands on experience in Big Data including Apache Spark Spark SQL and Spark Streaming Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Worked with Spark engine to process large scale data and experience to create Spark RDD and developing Spark Streaming jobs by using RDDs and leverage SparkShell Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Hands on experience in Apache Spark jobs using Scala in test environment for faster data processing and used SparkSQL for querying I have been experienced with SPARK SREAMING API to ingest data into SPARK ENGINE from KAFKA Worked on real time data integration using Kafka Storm data pipeline Spark streaming and HBase Experienced in implementing unified data platforms using Kafka producers consumers implement preprocessing using storm topologies Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Spark Good working experience on different file formats CSV Sequence files XML JSON PARQUET TEXTFILE AVRO ORC and different compression codecs GZIP SNAPPY LZO Hands on experience with NoSQL Databases like HBase Cassandra and relational databases like Oracle DB2 SQL SERVER and MySQL Expertise in job scheduling and monitoring tools like Oozie and ZooKeeper and experience in designing Oozie workflows for cleaning data and storing into Hive tables for quick analysis Strong experience in working with ELASTIC MAPREDUCE and setting up environments on Amazon AWS EC2 instances AZURE EMR and S3 Installed and configured JENKINS FOR AUTOMATING Deployments and providing automation solution Developed build and deployment scripts using ANT and MAVEN as build tools in JENKINS to move from one environment to other environments Extensive experience in ETL Data Ingestion InStream data processing Batch Analytics and Data Persistence Strategy Worked extensively with Dimensional Modeling Data Migration Data Cleansing Data Transformation and ETL Processes features for Data Warehouse System Experience with creating the TABLEAU dashboards with relational and multidimensional databases including Oracle MySQL and HIVE gathering and manipulating data from various sources Having experience in performance tuning dashboards and TABLEAU reports Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Expertise in design and development of Web Applications involving J2EE technologies with Java Spring EJB AJAX Servlets JSP Struts Web Services XML JMS JSP UNIX shell scripts SERVLETS MS SQL SERVER SOAP and RESTful web services Extensively development experience in different IDEs like Eclipse NetBeans Experience in core Java JDBC and proficient in using Java APIs for application development Experience in Deploying web application using application servers WebLogic Apache Tomcat WebSphere and JBOSS Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Work Experience Sr HadoopSpark Developer Sprint Overland Park KS February 2017 to Present Responsibilities Hands on experience in Spark and Spark Streaming creating RDD applying operations transformations and Actions Developed Spark applications using Scala for easy Hadoop transitions Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Spark code using Scala and SparkSQL for faster processing and testing Implemented Spark sample programs in python using pyspark Analyzed the SQL scripts and designed the solution to implement using pyspark Developed pyspark code to mimic the transformations performed in the onpremise environment Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Responsible for loading Data pipelines from web servers and Teradata using Sqoop with Kafka and Spark Streaming API Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Hive Populated HDFS and HBase with huge amounts of data using Apache Kafka Used Kafka to ingest data into Spark engine Configured deployed and maintained multinode Dev and Test Kafka Clusters Managing and scheduling Spark Jobs on a Hadoop Cluster using Oozie Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Designed and implemented Incremental Imports into Hive tables and writing Hive queries to run on TEZ Experienced data pipelines using Kafka and Akka for handling large terabytes of data Written shell scripts that run multiple Hive jobs which helps to automate different Hive tables incrementally which are used to generate different reports using Tableau for the Business use Experienced in Apache Spark for implementing advanced procedures like text analytics and processing using the inmemory computing capabilities written in Scala Developed Solr web apps to query and visualize and Solr indexed data from HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Involvement in creating custom UDFs for Pig and Hive to consolidate strategies and usefulness of Python into Pig Latin and HQL HiveQL Extensively worked on Text ORC Avro and Parquet file formats and compression techniques like Snappy Gzip and Zlib Implemented Hortonworks NiFi HDP 24 and recommended solution to inject data from multiple data sources to HDFS and Hive using NiFi Developed various data loading strategies and performed various transformations for analyzing the datasets by using Hortonworks Distribution for Hadoop ecosystem Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement and used Cassandra through Java services Experience in NoSQL ColumnOriented Databases like Cassandra and its Integration with Hadoop cluster Build servers using AWS importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection and open stack to provision new machines for clients Implemented AWS solutions using EC2 S3 RDS ECS EBS Elastic Load Balancer and Auto scaling groups Optimized volumes and EC2 instances Creating S3 buckets and managing policies for S3 buckets and utilized S3 bucket and Glacier for storage and backup AWS Performed AWS Cloud administration managing EC2 instances S3 SES and SNS services Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into HDFS using java and Talend Along with the Infrastructure team involved in design and developed Kafka and Storm based data pipeline ORM framework with spring framework for data persistence and transaction management Environment Hadoop Hive Map reduce Sqoop Kafka Spark Yarn Pig Cassandra Oozie shell Scripting Scala Maven Java JUnit agile methodologies NIFI MySQL Tableau AWS EC2 S3 Hortonworks power BI Solr Hadoop Developer Charter Communications St Louis MO August 2015 to January 2017 Description Its services for businesses include internet access data networking phone and wireless backhaul Charter chose Spectrum as the brand name for the combined cable internet voice and business services The combined company serves about more than 26 million residential and commercial customers across the US Responsibilities Worked on Hadoop cluster and data querying tools Hive to store and retrieve data While developing applications involved in complete Software Development Life Cycle SDLC Reviewing and managing Hadoop log files by consolidating logs from multiple machines using flume Developed Oozie workflow for scheduling ETL process and Hive Scripts Started using apache NiFi to copy the data from local file system to HDFS Involved in teams to analyze the Anomaly detection and ratings of data Implemented custom input format and record reader to read XML input efficiently using SAX parser Involved in writing queries in SparkSQL using Scala Worked with SPLUNK to analyze and visualize data Analyze database and compare it with other opensource NoSQL databases to find which one of them better suites the current requirement Integrated Cassandra as a distributed persistent metadata store to provide metadata resolution for network entities on the network Implemented Spark using Scala and SparkSQL for faster testing and processing of data Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Involved in working with Impala for data retrieval process Exported data from Impala to Tableau reporting tool created dashboards on live connection Designed multiple Python packages that were used within a large ETL process used to load 2TB of data from an existing Oracle database into a new PostgreSQL cluster Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Loaded data from Linux file system to HDFS and viceversa Developed UDFs using both DataFramesSQL and RDD in Spark for data Aggregation queries and reverting back into OLTP through Sqoop POC for enabling member and suspect search using Solr Worked on ETL methods for data extraction transformation and loading in corporatewide ETL Solutions and Data warehouse tools for reporting and data analysis Used CSVExcelStorage to parse with different delimiters in PIG Installed and monitored Hadoop ecosystems tools on multiple operating systems like Ubuntu CentOS Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Modified reports and Talend ETL jobs based on the feedback from QA testers and Users in development and staging environments Involved in setting QA environment by implementing pig and Sqoop scripts Got chance working on Apache NiFi like executing Spark script Sqoop scripts through NiFi worked on creating scatter and gather pattern in NiFi ingesting data from Postgres to HDFS Fetching Hive metadata and storing in HDFS created a custom NiFi processor for filtering text from Flow files etc Responsible for designing and implementing ETL process using Talend to load data from Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systemsmainframe and viceversa Developed Pig Latin scripts to do operations of sorting joining and filtering enterprise data Implemented test scripts to support test driven development and integration Developed multiple MapReduce jobs in java to clean datasets Involved in loading data from Linux file systems servers java web services using Kafka producers and consumers Involved in developing code to write canonical model JSON records from numerous input sources to Kafka Queues Performed streaming of data into Apache ignite by setting up cache for efficient data analysis Collected the logs data from web servers and integrated in to HDFS using Flume Developed UNIX shell scripts for creating the reports from Hive data Manipulate serialize model data in multiple forms like JSON XML Involved in setting up MapReduce 1 and MapReduce 2 Prepared Avro schema files for generating Hive tables and Created Hive tables and loaded the data in to tables and query data using HQL Installed and Configured Hadoop cluster using Amazon Web Services AWS for POC purposes Environment Hadoop MapReduce 2 YARN Nifi HDFS PIG Hive Flume Cassandra Eclipse Ignite Core Java Sqoop Spark Splunk Maven SparkSQl Cloudera SolrTalend Linux shell scripting JavaHadoop Developer Grubhub Chicago IL June 2013 to July 2015 Description Grubhub is an online and mobile foodordering company that connects diners with local restaurants Based in Chicago the company has more than 918 million active diners and more than 55000 restaurant partners in over 1100 cities across the United States and the United Kingdom The idea of Integrated Data Ware House Project is to ingest data from different multiple sources to Hadoop Data Lake perform transformations on it according to business requirements and loading the data to external systems Responsibilities Exported data from DB2 to HDFS using Sqoop and Developed MapReduce jobs using Java API Designed and implemented Java engine and API to perform direct calls from frontend JavaScript ExtJS to serverside Java methods ExtDirect Used Spring AOP to implement Distributed declarative transaction throughout the application Designed and developed Java batch programs in Spring Batch Worked on Data Lake architecture to build a reliable scalable analytics platform to meet batch interactive and online analytics requirements Concerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented optimized map joins to get data from different sources to perform cleaning operations before applying the algorithms Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Used various compression codecs to effectively compress the data in HDFS Used Avro SerDes for serialization and deserialization and also implemented hive custom UDFs involving date functions Responsible for troubleshooting issues in the execution of MapReduce jobs by inspecting and reviewing log files Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Installed and configured Pig and wrote Pig Latin scripts Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Developed workflowusing Oozie for running MapReduce jobs and Hive Queries Done the work in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP Involved in loading data from UNIX file system to HDFS Created java operators to process data using DAG streams and load data to HDFS Assisted in exporting analyzed data to relational databases using Sqoop Involved in Develop monitoring and performance metrics for Hadoop clusters Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment Hadoop HDFS Hive Flume Sqoop HBase PIG Eclipse Spark My SQL and Ubuntu Zookeeper Maven Jenkins Java JDK 16 Oracle10g Java Developer Avineon Inc McLean VA September 2011 to May 2013 Description Avineon has been providing high quality information technology IT solutions to clients operating in both government and business sectors Its aim is to meet our clients business and organizational objectives with the help of easytouse featurerich IT solutions Avineon offers IT services in Software Solutions Security Engineering Cloud Computing and Strategic Business Services and IT Service Management Responsibilities Involved in the design development and deployment of the Application using JavaJ2EE Technologies Performed Requirements gathering and analysis and prepared Requirements Specifications document Provided high level systems design specifying the class diagrams sequence diagrams and activity diagrams Involved in designing user interactive web pages as the frontend part of the web application using various web technologies like HTML JavaScript Angular JS AJAX and implemented CSS for better appearance and feel Integrated AEM to the existing web application and created AEM components using JavaScript CSS and HTML Programmed Oracle SQL TSQL Stored Procedures Functions Triggers and Packages as back end processes to create and update staging tables log and audit tables and creating primary keys Provided further Maintenance and support this involves working with the Client and solving their problems which include major Bug fixing Deployed and tested the application using Tomcat web server Analysis of the specifications provided by the clients Developed JAVABEAN components utilizing AWT and SWING classes Extensively used Transformations like Aggregator Router Joiner Expression Lookup Update Strategy and Sequence Generator Used Exception handling and Multithreading for the optimum performance of the application Used the Core Java concepts to implement the Business Logic Provided on call support based on the priority of the issues Designed and implemented a generic parser framework using SAX parser to parse XML documents which stores SQL Perform Functional testing Performance testing Integration testing Regression testing Smoke testing and User Acceptance Testing UAT Environment Core Java Servlets struts JSP XML XSLT JavaScript Apache Oracle 10g11g Jr Java Developer Emorphosys Hyderabad Telangana July 2009 to August 2011 Description This is a business telephone application known as Softphone which is designed to work with IPbased phone systems and Local Area Network or Wide Area Network WAN The server supports the Softphone features like Call Conference Call Transfer and Call Forward The Softphone provides Voice over IP VoIP service using a telephony server It is used to make and take calls through computer using a headset or handset connected to Avaya Hard phone Responsibilities Documented in SharePoint and Confluence of projects Created the PTO Permit To Operate of the project to provide the supported documents before the development phase Developed the Web Application using Spring MVC To develop the web view of the application used CSS HTML JSP and Java Script Used Control M to configure and schedule the Batch Jobs IT Service Management ITSM is used to plan deliver operate and control IT services offered In the TDD Development process of the project created and tested the functionality of the classes with mocking framework Mockito For writing the code used Eclipse Neon IDE for the software development For build and deploy the application to development environment used Maven Managed the different versions of the source code with TortoiseSVN client For Agile Project Management used JIRA Created Application Flow Diagram using Visio at the time creation of PTO For continuous delivery testing and deployment used Jenkins Environment JAVA 18 Spring MVC 32 Hibernate 42 JavaScript JSON 22 Control M JIRA ITSM Eclipse Neon Maven 339 Jenkins Visio Mockito 22 SVN Linux Windows 7 Education Bachelor of Technology in Electronics and Communication Engineering University of Illinois UrbanaChampaign IL Masters of Science in Computer Science Governors State University Chicago IL Skills JAVA 9 years SQL 6 years ORACLE 5 years Hadoop 5 years HADOOP 5 years Additional Information Operating Systems Windows Linux UNIX Ubuntu Centos Programming or Scripting Languages C C Core JavaJ2EE Unix Shell Scripting Python SQL Pig Latin Hive QL Scala Hadoop Distributions ClouderaCDH4CDH5 Hortonworks HDP25 IDEGUI Eclipse32 IntelliJ Scala IDE Build Tools Jenkins Maven ANT Database Microsoft SQL Server MS SQL Oracle 11g10g DB2 MySQL MSAccess MSAccess NoSQL HBase Cassandra Cloud Computing Tools Amazon AWSAZURE Versioning Tools JIRA CVS SVN and GitHub SDLC Methodologies Agile Scrum Waterfall Model",
    "entities": [
        "Tested Apache TEZ",
        "Implemented Spark",
        "Node",
        "FLUME",
        "SERVLETS MS SQL SERVER",
        "Resource",
        "Infrastructure",
        "ETL Solutions",
        "Oracle MySQL",
        "HADOOP",
        "Developed JAVABEAN",
        "Spark Streaming Implemented",
        "HDFS",
        "UNIX",
        "SQL Oracle",
        "Apache Sqoop",
        "HTTP",
        "Data Lake",
        "NiFi",
        "Data mart",
        "JSON",
        "Created the PTO Permit",
        "CSS HTML",
        "Strategic Business Services",
        "SQL Perform Functional",
        "Amazon AWS EC2",
        "Local Area Network",
        "Scala Hadoop Distributions",
        "Amazon Web Services AWS",
        "Cloudera Hadoop Clusters",
        "RDD",
        "Hadoop",
        "Sqoop Involved",
        "HDFS Involved",
        "XML",
        "TDD Development",
        "Integrated Data Ware House Project",
        "WebLogic",
        "Talend Along",
        "Python Script Experience",
        "the Business Logic Provided",
        "SparkSQl",
        "Spark Good",
        "HBase",
        "Developed Oozie",
        "SPLUNK",
        "Apache Spark",
        "HDFS Created User Defined Functions",
        "Amazon",
        "Data Warehouse System",
        "Dimensional Modeling Data Migration Data Cleansing Data Transformation",
        "WebSphere",
        "the US Responsibilities Worked on Hadoop",
        "Implemented Hive",
        "Python Worked on",
        "SparkSQL",
        "ETL Data Ingestion InStream",
        "Kerberos",
        "JavaHadoop Developer",
        "Oozie Experienced",
        "HTML Programmed Oracle SQL TSQL Stored Procedures Functions Triggers",
        "Node Data",
        "Mockito",
        "NIFI",
        "Maven Managed",
        "Hadoop Log",
        "Cassandra Involvement",
        "IT Service Management Responsibilities Involved",
        "Client",
        "SDLC Agile Waterfall",
        "Spark Hands",
        "Oracle MySQL DB2 MS",
        "Hadoop Data Lake",
        "GZIP SNAPPY LZO Hands",
        "Develop",
        "JIRA Created Application Flow Diagram",
        "Linux",
        "JSP",
        "log files",
        "HDFS Assisted",
        "Control M JIRA ITSM",
        "SQOOP Involved",
        "Worked",
        "SQL Queries",
        "SPARK ENGINE",
        "Spark Streaming",
        "Talend",
        "Additional Information Operating Systems",
        "BI Solr Hadoop Developer Charter Communications",
        "RDS",
        "Software Development Life Cycle SDLC Reviewing",
        "Incremental Imports",
        "Spark",
        "JavaScript ExtJS",
        "SparkShell",
        "TABLEAU",
        "Created Hive",
        "Description Avineon",
        "Present Responsibilities Hands",
        "the Application using JavaJ2EE Technologies Performed Requirements",
        "API",
        "Hadoop Gen1",
        "Hortonworks Distribution for Hadoop",
        "Sqoop",
        "QA",
        "TEZ Experienced",
        "Sr HadoopSpark Developer",
        "NiFi Developed",
        "Storm",
        "Created",
        "AWS",
        "Scala",
        "Agile Project Management",
        "Requirements Specifications",
        "Anomaly",
        "Implemented AWS",
        "Postgres",
        "Oracle DB",
        "SPARK SREAMING API",
        "PIG",
        "JENKINS",
        "Spark Involved",
        "SAX",
        "HDFS Job Tracker Task Tracker",
        "java",
        "Oozie",
        "Environment Hadoop MapReduce 2",
        "the United Kingdom",
        "SQL",
        "OLTP",
        "MAVEN",
        "AEM",
        "the Batch Jobs IT Service Management ITSM",
        "User Acceptance Testing UAT Environment",
        "Relational Database Systems",
        "HQL Installed and Configured Hadoop",
        "Based",
        "Object Oriented Analysis Design OOAD",
        "Chicago",
        "the United States",
        "AWT",
        "Big Data",
        "Hive",
        "HDFS Created",
        "Scala Modified",
        "Call Conference Call Transfer",
        "Avaya Hard phone Responsibilities Documented in SharePoint",
        "DAG",
        "ZooKeeper",
        "DataFramesSQL",
        "ETL",
        "Oracle DB2 SQL SERVER",
        "Build",
        "Maven",
        "Hibernate",
        "Impala",
        "Spark SQL",
        "Talend ETL",
        "GitHub SDLC Methodologies Agile Scrum Waterfall Model",
        "ANT",
        "Analysis Design Development Implementation Maintenance",
        "IP VoIP",
        "HBase Experienced",
        "Horton Works Hadoop Distributions Capable",
        "SVN",
        "Software Solutions Security Engineering",
        "Hadoop Components",
        "CSS",
        "Integration with Hadoop",
        "Developed MapReduce",
        "BIG DATA",
        "PIG Installed",
        "Scala Developed Spark",
        "Data warehouses Work Experience Sr HadoopSpark Developer Sprint",
        "Tomcat",
        "Data",
        "REST",
        "Relational Database",
        "MapReduce",
        "JavaScript Apache Oracle",
        "TortoiseSVN",
        "UML Methodology",
        "RDBMS",
        "JSP XML",
        "Tableau",
        "Java Developer Avineon Inc McLean VA",
        "Communication Engineering University of Illinois UrbanaChampaign IL Masters of Science in",
        "Technical Design",
        "IPbased",
        "Teradata",
        "Hive data Manipulate",
        "Bucketing"
    ],
    "experience": "Experience in working with Amazon EMR Cloudera CDH4CDH5 and Horton Works Hadoop Distributions Capable of processing large sets of structured semistructured and unstructured data and supporting systems application architecture Extensively used Apache Sqoop for efficiently importing and exporting data from HDFS to Relational Database Systems and from RDBMS to HDFS Worked on data load from various sources ie Oracle MySQL DB2 MS SQL Server Cassandra Hadoop using Sqoop and Python Script Experience in developing data pipeline using Sqoop and Flume to extract the data from weblogs and store in HDFS Experience in managing and reviewing Hadoop Log files using FLUME and Kafka and also developed the Pig UDFs and Hive UDFs to preprocess the data for analysis Worked on Impala for Massive parallel processing of Hive queries Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Efficient in working with Hive data warehouse tool creating tables data distributing by implementing Partitioning and Bucketing strategy writing and optimizing the HiveQL queries Experience in ingestion storage querying processing and analysis of Big Data with hands on experience in Big Data including Apache Spark Spark SQL and Spark Streaming Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Worked with Spark engine to process large scale data and experience to create Spark RDD and developing Spark Streaming jobs by using RDDs and leverage SparkShell Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Hands on experience in Apache Spark jobs using Scala in test environment for faster data processing and used SparkSQL for querying I have been experienced with SPARK SREAMING API to ingest data into SPARK ENGINE from KAFKA Worked on real time data integration using Kafka Storm data pipeline Spark streaming and HBase Experienced in implementing unified data platforms using Kafka producers consumers implement preprocessing using storm topologies Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Spark Good working experience on different file formats CSV Sequence files XML JSON PARQUET TEXTFILE AVRO ORC and different compression codecs GZIP SNAPPY LZO Hands on experience with NoSQL Databases like HBase Cassandra and relational databases like Oracle DB2 SQL SERVER and MySQL Expertise in job scheduling and monitoring tools like Oozie and ZooKeeper and experience in designing Oozie workflows for cleaning data and storing into Hive tables for quick analysis Strong experience in working with ELASTIC MAPREDUCE and setting up environments on Amazon AWS EC2 instances AZURE EMR and S3 Installed and configured JENKINS FOR AUTOMATING Deployments and providing automation solution Developed build and deployment scripts using ANT and MAVEN as build tools in JENKINS to move from one environment to other environments Extensive experience in ETL Data Ingestion InStream data processing Batch Analytics and Data Persistence Strategy Worked extensively with Dimensional Modeling Data Migration Data Cleansing Data Transformation and ETL Processes features for Data Warehouse System Experience with creating the TABLEAU dashboards with relational and multidimensional databases including Oracle MySQL and HIVE gathering and manipulating data from various sources Having experience in performance tuning dashboards and TABLEAU reports Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Expertise in design and development of Web Applications involving J2EE technologies with Java Spring EJB AJAX Servlets JSP Struts Web Services XML JMS JSP UNIX shell scripts SERVLETS MS SQL SERVER SOAP and RESTful web services Extensively development experience in different IDEs like Eclipse NetBeans Experience in core Java JDBC and proficient in using Java APIs for application development Experience in Deploying web application using application servers WebLogic Apache Tomcat WebSphere and JBOSS Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Work Experience Sr HadoopSpark Developer Sprint Overland Park KS February 2017 to Present Responsibilities Hands on experience in Spark and Spark Streaming creating RDD applying operations transformations and Actions Developed Spark applications using Scala for easy Hadoop transitions Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Spark code using Scala and SparkSQL for faster processing and testing Implemented Spark sample programs in python using pyspark Analyzed the SQL scripts and designed the solution to implement using pyspark Developed pyspark code to mimic the transformations performed in the onpremise environment Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Responsible for loading Data pipelines from web servers and Teradata using Sqoop with Kafka and Spark Streaming API Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Hive Populated HDFS and HBase with huge amounts of data using Apache Kafka Used Kafka to ingest data into Spark engine Configured deployed and maintained multinode Dev and Test Kafka Clusters Managing and scheduling Spark Jobs on a Hadoop Cluster using Oozie Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Designed and implemented Incremental Imports into Hive tables and writing Hive queries to run on TEZ Experienced data pipelines using Kafka and Akka for handling large terabytes of data Written shell scripts that run multiple Hive jobs which helps to automate different Hive tables incrementally which are used to generate different reports using Tableau for the Business use Experienced in Apache Spark for implementing advanced procedures like text analytics and processing using the inmemory computing capabilities written in Scala Developed Solr web apps to query and visualize and Solr indexed data from HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Involvement in creating custom UDFs for Pig and Hive to consolidate strategies and usefulness of Python into Pig Latin and HQL HiveQL Extensively worked on Text ORC Avro and Parquet file formats and compression techniques like Snappy Gzip and Zlib Implemented Hortonworks NiFi HDP 24 and recommended solution to inject data from multiple data sources to HDFS and Hive using NiFi Developed various data loading strategies and performed various transformations for analyzing the datasets by using Hortonworks Distribution for Hadoop ecosystem Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement and used Cassandra through Java services Experience in NoSQL ColumnOriented Databases like Cassandra and its Integration with Hadoop cluster Build servers using AWS importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection and open stack to provision new machines for clients Implemented AWS solutions using EC2 S3 RDS ECS EBS Elastic Load Balancer and Auto scaling groups Optimized volumes and EC2 instances Creating S3 buckets and managing policies for S3 buckets and utilized S3 bucket and Glacier for storage and backup AWS Performed AWS Cloud administration managing EC2 instances S3 SES and SNS services Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into HDFS using java and Talend Along with the Infrastructure team involved in design and developed Kafka and Storm based data pipeline ORM framework with spring framework for data persistence and transaction management Environment Hadoop Hive Map reduce Sqoop Kafka Spark Yarn Pig Cassandra Oozie shell Scripting Scala Maven Java JUnit agile methodologies NIFI MySQL Tableau AWS EC2 S3 Hortonworks power BI Solr Hadoop Developer Charter Communications St Louis MO August 2015 to January 2017 Description Its services for businesses include internet access data networking phone and wireless backhaul Charter chose Spectrum as the brand name for the combined cable internet voice and business services The combined company serves about more than 26 million residential and commercial customers across the US Responsibilities Worked on Hadoop cluster and data querying tools Hive to store and retrieve data While developing applications involved in complete Software Development Life Cycle SDLC Reviewing and managing Hadoop log files by consolidating logs from multiple machines using flume Developed Oozie workflow for scheduling ETL process and Hive Scripts Started using apache NiFi to copy the data from local file system to HDFS Involved in teams to analyze the Anomaly detection and ratings of data Implemented custom input format and record reader to read XML input efficiently using SAX parser Involved in writing queries in SparkSQL using Scala Worked with SPLUNK to analyze and visualize data Analyze database and compare it with other opensource NoSQL databases to find which one of them better suites the current requirement Integrated Cassandra as a distributed persistent metadata store to provide metadata resolution for network entities on the network Implemented Spark using Scala and SparkSQL for faster testing and processing of data Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Involved in working with Impala for data retrieval process Exported data from Impala to Tableau reporting tool created dashboards on live connection Designed multiple Python packages that were used within a large ETL process used to load 2 TB of data from an existing Oracle database into a new PostgreSQL cluster Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Loaded data from Linux file system to HDFS and viceversa Developed UDFs using both DataFramesSQL and RDD in Spark for data Aggregation queries and reverting back into OLTP through Sqoop POC for enabling member and suspect search using Solr Worked on ETL methods for data extraction transformation and loading in corporatewide ETL Solutions and Data warehouse tools for reporting and data analysis Used CSVExcelStorage to parse with different delimiters in PIG Installed and monitored Hadoop ecosystems tools on multiple operating systems like Ubuntu CentOS Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Modified reports and Talend ETL jobs based on the feedback from QA testers and Users in development and staging environments Involved in setting QA environment by implementing pig and Sqoop scripts Got chance working on Apache NiFi like executing Spark script Sqoop scripts through NiFi worked on creating scatter and gather pattern in NiFi ingesting data from Postgres to HDFS Fetching Hive metadata and storing in HDFS created a custom NiFi processor for filtering text from Flow files etc Responsible for designing and implementing ETL process using Talend to load data from Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systemsmainframe and viceversa Developed Pig Latin scripts to do operations of sorting joining and filtering enterprise data Implemented test scripts to support test driven development and integration Developed multiple MapReduce jobs in java to clean datasets Involved in loading data from Linux file systems servers java web services using Kafka producers and consumers Involved in developing code to write canonical model JSON records from numerous input sources to Kafka Queues Performed streaming of data into Apache ignite by setting up cache for efficient data analysis Collected the logs data from web servers and integrated in to HDFS using Flume Developed UNIX shell scripts for creating the reports from Hive data Manipulate serialize model data in multiple forms like JSON XML Involved in setting up MapReduce 1 and MapReduce 2 Prepared Avro schema files for generating Hive tables and Created Hive tables and loaded the data in to tables and query data using HQL Installed and Configured Hadoop cluster using Amazon Web Services AWS for POC purposes Environment Hadoop MapReduce 2 YARN Nifi HDFS PIG Hive Flume Cassandra Eclipse Ignite Core Java Sqoop Spark Splunk Maven SparkSQl Cloudera SolrTalend Linux shell scripting JavaHadoop Developer Grubhub Chicago IL June 2013 to July 2015 Description Grubhub is an online and mobile foodordering company that connects diners with local restaurants Based in Chicago the company has more than 918 million active diners and more than 55000 restaurant partners in over 1100 cities across the United States and the United Kingdom The idea of Integrated Data Ware House Project is to ingest data from different multiple sources to Hadoop Data Lake perform transformations on it according to business requirements and loading the data to external systems Responsibilities Exported data from DB2 to HDFS using Sqoop and Developed MapReduce jobs using Java API Designed and implemented Java engine and API to perform direct calls from frontend JavaScript ExtJS to serverside Java methods ExtDirect Used Spring AOP to implement Distributed declarative transaction throughout the application Designed and developed Java batch programs in Spring Batch Worked on Data Lake architecture to build a reliable scalable analytics platform to meet batch interactive and online analytics requirements Concerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented optimized map joins to get data from different sources to perform cleaning operations before applying the algorithms Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Used various compression codecs to effectively compress the data in HDFS Used Avro SerDes for serialization and deserialization and also implemented hive custom UDFs involving date functions Responsible for troubleshooting issues in the execution of MapReduce jobs by inspecting and reviewing log files Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Installed and configured Pig and wrote Pig Latin scripts Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Developed workflowusing Oozie for running MapReduce jobs and Hive Queries Done the work in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP Involved in loading data from UNIX file system to HDFS Created java operators to process data using DAG streams and load data to HDFS Assisted in exporting analyzed data to relational databases using Sqoop Involved in Develop monitoring and performance metrics for Hadoop clusters Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment Hadoop HDFS Hive Flume Sqoop HBase PIG Eclipse Spark My SQL and Ubuntu Zookeeper Maven Jenkins Java JDK 16 Oracle10 g Java Developer Avineon Inc McLean VA September 2011 to May 2013 Description Avineon has been providing high quality information technology IT solutions to clients operating in both government and business sectors Its aim is to meet our clients business and organizational objectives with the help of easytouse featurerich IT solutions Avineon offers IT services in Software Solutions Security Engineering Cloud Computing and Strategic Business Services and IT Service Management Responsibilities Involved in the design development and deployment of the Application using JavaJ2EE Technologies Performed Requirements gathering and analysis and prepared Requirements Specifications document Provided high level systems design specifying the class diagrams sequence diagrams and activity diagrams Involved in designing user interactive web pages as the frontend part of the web application using various web technologies like HTML JavaScript Angular JS AJAX and implemented CSS for better appearance and feel Integrated AEM to the existing web application and created AEM components using JavaScript CSS and HTML Programmed Oracle SQL TSQL Stored Procedures Functions Triggers and Packages as back end processes to create and update staging tables log and audit tables and creating primary keys Provided further Maintenance and support this involves working with the Client and solving their problems which include major Bug fixing Deployed and tested the application using Tomcat web server Analysis of the specifications provided by the clients Developed JAVABEAN components utilizing AWT and SWING classes Extensively used Transformations like Aggregator Router Joiner Expression Lookup Update Strategy and Sequence Generator Used Exception handling and Multithreading for the optimum performance of the application Used the Core Java concepts to implement the Business Logic Provided on call support based on the priority of the issues Designed and implemented a generic parser framework using SAX parser to parse XML documents which stores SQL Perform Functional testing Performance testing Integration testing Regression testing Smoke testing and User Acceptance Testing UAT Environment Core Java Servlets struts JSP XML XSLT JavaScript Apache Oracle 10g11 g Jr Java Developer Emorphosys Hyderabad Telangana July 2009 to August 2011 Description This is a business telephone application known as Softphone which is designed to work with IPbased phone systems and Local Area Network or Wide Area Network WAN The server supports the Softphone features like Call Conference Call Transfer and Call Forward The Softphone provides Voice over IP VoIP service using a telephony server It is used to make and take calls through computer using a headset or handset connected to Avaya Hard phone Responsibilities Documented in SharePoint and Confluence of projects Created the PTO Permit To Operate of the project to provide the supported documents before the development phase Developed the Web Application using Spring MVC To develop the web view of the application used CSS HTML JSP and Java Script Used Control M to configure and schedule the Batch Jobs IT Service Management ITSM is used to plan deliver operate and control IT services offered In the TDD Development process of the project created and tested the functionality of the classes with mocking framework Mockito For writing the code used Eclipse Neon IDE for the software development For build and deploy the application to development environment used Maven Managed the different versions of the source code with TortoiseSVN client For Agile Project Management used JIRA Created Application Flow Diagram using Visio at the time creation of PTO For continuous delivery testing and deployment used Jenkins Environment JAVA 18 Spring MVC 32 Hibernate 42 JavaScript JSON 22 Control M JIRA ITSM Eclipse Neon Maven 339 Jenkins Visio Mockito 22 SVN Linux Windows 7 Education Bachelor of Technology in Electronics and Communication Engineering University of Illinois UrbanaChampaign IL Masters of Science in Computer Science Governors State University Chicago IL Skills JAVA 9 years SQL 6 years ORACLE 5 years Hadoop 5 years HADOOP 5 years Additional Information Operating Systems Windows Linux UNIX Ubuntu Centos Programming or Scripting Languages C C Core JavaJ2EE Unix Shell Scripting Python SQL Pig Latin Hive QL Scala Hadoop Distributions ClouderaCDH4CDH5 Hortonworks HDP25 IDEGUI Eclipse32 IntelliJ Scala IDE Build Tools Jenkins Maven ANT Database Microsoft SQL Server MS SQL Oracle 11g10 g DB2 MySQL MSAccess MSAccess NoSQL HBase Cassandra Cloud Computing Tools Amazon AWSAZURE Versioning Tools JIRA CVS SVN and GitHub SDLC Methodologies Agile Scrum Waterfall Model",
    "extracted_keywords": [
        "Sr",
        "HadoopSpark",
        "Developer",
        "Sr",
        "HadoopSpark",
        "span",
        "lDeveloperspan",
        "Sr",
        "HadoopSpark",
        "Developer",
        "Sprint",
        "Indianapolis",
        "IT",
        "experience",
        "years",
        "Analysis",
        "Design",
        "Development",
        "Implementation",
        "Maintenance",
        "Support",
        "experience",
        "methods",
        "Big",
        "Data",
        "technologies",
        "Big",
        "Data",
        "processing",
        "requirement",
        "years",
        "experience",
        "BIG",
        "DATA",
        "HADOOP",
        "framework",
        "technologies",
        "HDFS",
        "Map",
        "HIVE",
        "PIG",
        "YARN",
        "APACHE",
        "SPARK",
        "KAFKA",
        "OOZIE",
        "SQOOP",
        "ZOOKEEPER",
        "NoSQL",
        "Databases",
        "HBase",
        "Cassandra",
        "Hadoop",
        "Gen1",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Resource",
        "Manager",
        "YARN",
        "Experience",
        "Amazon",
        "EMR",
        "Cloudera",
        "CDH4CDH5",
        "Horton",
        "Works",
        "Hadoop",
        "Distributions",
        "sets",
        "data",
        "systems",
        "application",
        "architecture",
        "Apache",
        "Sqoop",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "RDBMS",
        "HDFS",
        "data",
        "load",
        "sources",
        "Oracle",
        "MySQL",
        "DB2",
        "MS",
        "SQL",
        "Server",
        "Cassandra",
        "Hadoop",
        "Sqoop",
        "Python",
        "Script",
        "Experience",
        "data",
        "pipeline",
        "Sqoop",
        "Flume",
        "data",
        "weblogs",
        "HDFS",
        "Experience",
        "Hadoop",
        "Log",
        "files",
        "FLUME",
        "Kafka",
        "Pig",
        "UDFs",
        "Hive",
        "UDFs",
        "data",
        "analysis",
        "Impala",
        "processing",
        "Hive",
        "queries",
        "HIVE",
        "PIG",
        "core",
        "functionality",
        "custom",
        "User",
        "Defined",
        "Functions",
        "UDF",
        "User",
        "Defined",
        "TableGenerating",
        "Functions",
        "UDTF",
        "User",
        "Defined",
        "Aggregating",
        "Functions",
        "UDAF",
        "Hive",
        "Pig",
        "Efficient",
        "Hive",
        "data",
        "warehouse",
        "tool",
        "tables",
        "data",
        "Partitioning",
        "strategy",
        "writing",
        "HiveQL",
        "queries",
        "Experience",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "Big",
        "Data",
        "hands",
        "experience",
        "Big",
        "Data",
        "Apache",
        "Spark",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Spark",
        "engine",
        "scale",
        "data",
        "experience",
        "Spark",
        "RDD",
        "Spark",
        "Streaming",
        "jobs",
        "RDDs",
        "leverage",
        "SparkShell",
        "experience",
        "RDD",
        "architecture",
        "Spark",
        "operations",
        "RDD",
        "transformations",
        "actions",
        "Spark",
        "Hands",
        "experience",
        "Apache",
        "Spark",
        "jobs",
        "Scala",
        "test",
        "environment",
        "data",
        "processing",
        "SparkSQL",
        "SPARK",
        "SREAMING",
        "API",
        "data",
        "SPARK",
        "ENGINE",
        "KAFKA",
        "time",
        "data",
        "integration",
        "Kafka",
        "Storm",
        "data",
        "pipeline",
        "Spark",
        "streaming",
        "HBase",
        "data",
        "platforms",
        "Kafka",
        "producers",
        "consumers",
        "storm",
        "topologies",
        "Exposure",
        "Data",
        "Lake",
        "Implementation",
        "Apache",
        "Spark",
        "Data",
        "pipe",
        "lines",
        "business",
        "logics",
        "Spark",
        "Good",
        "working",
        "experience",
        "file",
        "formats",
        "CSV",
        "Sequence",
        "files",
        "XML",
        "PARQUET",
        "TEXTFILE",
        "AVRO",
        "ORC",
        "compression",
        "codecs",
        "GZIP",
        "LZO",
        "Hands",
        "experience",
        "NoSQL",
        "Databases",
        "HBase",
        "Cassandra",
        "databases",
        "Oracle",
        "DB2",
        "SQL",
        "SERVER",
        "MySQL",
        "Expertise",
        "job",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "ZooKeeper",
        "experience",
        "Oozie",
        "workflows",
        "data",
        "Hive",
        "tables",
        "analysis",
        "experience",
        "ELASTIC",
        "MAPREDUCE",
        "environments",
        "Amazon",
        "AWS",
        "EC2",
        "instances",
        "EMR",
        "S3",
        "Installed",
        "JENKINS",
        "AUTOMATING",
        "Deployments",
        "automation",
        "solution",
        "build",
        "deployment",
        "scripts",
        "ANT",
        "MAVEN",
        "build",
        "tools",
        "JENKINS",
        "environment",
        "environments",
        "experience",
        "ETL",
        "Data",
        "Ingestion",
        "InStream",
        "data",
        "Batch",
        "Analytics",
        "Data",
        "Persistence",
        "Strategy",
        "Dimensional",
        "Modeling",
        "Data",
        "Migration",
        "Data",
        "Cleansing",
        "Data",
        "Transformation",
        "ETL",
        "Processes",
        "features",
        "Data",
        "Warehouse",
        "System",
        "Experience",
        "TABLEAU",
        "dashboards",
        "databases",
        "Oracle",
        "MySQL",
        "HIVE",
        "gathering",
        "data",
        "sources",
        "experience",
        "performance",
        "dashboards",
        "TABLEAU",
        "Experience",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "authorization",
        "infrastructure",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "software",
        "UML",
        "Methodology",
        "knowledge",
        "J2EE",
        "design",
        "patterns",
        "Core",
        "Java",
        "design",
        "patterns",
        "Expertise",
        "design",
        "development",
        "Web",
        "Applications",
        "J2EE",
        "technologies",
        "Java",
        "Spring",
        "EJB",
        "AJAX",
        "Servlets",
        "JSP",
        "Struts",
        "Web",
        "Services",
        "XML",
        "JMS",
        "JSP",
        "UNIX",
        "shell",
        "scripts",
        "SERVLETS",
        "MS",
        "SQL",
        "SERVER",
        "SOAP",
        "web",
        "services",
        "development",
        "experience",
        "IDEs",
        "Eclipse",
        "NetBeans",
        "Experience",
        "core",
        "Java",
        "JDBC",
        "Java",
        "APIs",
        "application",
        "development",
        "Experience",
        "Deploying",
        "web",
        "application",
        "application",
        "servers",
        "WebLogic",
        "Apache",
        "Tomcat",
        "WebSphere",
        "JBOSS",
        "Experience",
        "stages",
        "SDLC",
        "Agile",
        "Waterfall",
        "Technical",
        "Design",
        "document",
        "Development",
        "Testing",
        "Implementation",
        "Enterprise",
        "level",
        "Data",
        "mart",
        "Data",
        "Work",
        "Experience",
        "Sr",
        "HadoopSpark",
        "Developer",
        "Sprint",
        "Overland",
        "Park",
        "KS",
        "February",
        "Present",
        "Responsibilities",
        "Hands",
        "experience",
        "Spark",
        "Spark",
        "Streaming",
        "RDD",
        "operations",
        "transformations",
        "Actions",
        "Spark",
        "applications",
        "Scala",
        "Hadoop",
        "transitions",
        "Spark",
        "SparkSQL",
        "parquet",
        "data",
        "tables",
        "hive",
        "Scala",
        "API",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "processing",
        "testing",
        "Spark",
        "sample",
        "programs",
        "python",
        "pyspark",
        "SQL",
        "scripts",
        "solution",
        "pyspark",
        "pyspark",
        "code",
        "transformations",
        "onpremise",
        "environment",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Data",
        "pipelines",
        "web",
        "servers",
        "Teradata",
        "Sqoop",
        "Kafka",
        "Spark",
        "Streaming",
        "API",
        "Kafka",
        "producer",
        "consumers",
        "Cassandra",
        "clients",
        "Spark",
        "components",
        "HDFS",
        "Hive",
        "Populated",
        "HDFS",
        "HBase",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "Kafka",
        "data",
        "Spark",
        "engine",
        "multinode",
        "Dev",
        "Test",
        "Kafka",
        "Clusters",
        "scheduling",
        "Spark",
        "Jobs",
        "Hadoop",
        "Cluster",
        "Oozie",
        "scripting",
        "language",
        "Python",
        "scripts",
        "Python",
        "scripts",
        "vulnerabilities",
        "SQL",
        "Queries",
        "SQL",
        "injection",
        "permission",
        "checks",
        "performance",
        "analysis",
        "Tested",
        "Apache",
        "TEZ",
        "framework",
        "performance",
        "batch",
        "data",
        "processing",
        "applications",
        "Pig",
        "Hive",
        "jobs",
        "Incremental",
        "Imports",
        "Hive",
        "tables",
        "Hive",
        "queries",
        "TEZ",
        "data",
        "pipelines",
        "Kafka",
        "Akka",
        "terabytes",
        "data",
        "shell",
        "scripts",
        "Hive",
        "jobs",
        "Hive",
        "tables",
        "reports",
        "Tableau",
        "Business",
        "use",
        "Apache",
        "Spark",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Scala",
        "Developed",
        "Solr",
        "web",
        "apps",
        "query",
        "visualize",
        "Solr",
        "data",
        "HDFS",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Python",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "data",
        "Hive",
        "tables",
        "prep",
        "data",
        "AWS",
        "S3",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "Cassandra",
        "Involvement",
        "custom",
        "UDFs",
        "Pig",
        "Hive",
        "strategies",
        "usefulness",
        "Python",
        "Pig",
        "Latin",
        "HQL",
        "HiveQL",
        "Text",
        "ORC",
        "Avro",
        "Parquet",
        "file",
        "formats",
        "compression",
        "techniques",
        "Snappy",
        "Gzip",
        "Zlib",
        "Implemented",
        "Hortonworks",
        "NiFi",
        "HDP",
        "solution",
        "data",
        "data",
        "sources",
        "HDFS",
        "Hive",
        "NiFi",
        "data",
        "loading",
        "strategies",
        "transformations",
        "datasets",
        "Hortonworks",
        "Distribution",
        "Hadoop",
        "ecosystem",
        "data",
        "RDBMS",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "Cassandra",
        "Java",
        "services",
        "Experience",
        "NoSQL",
        "ColumnOriented",
        "Databases",
        "Cassandra",
        "Integration",
        "Hadoop",
        "cluster",
        "Build",
        "servers",
        "AWS",
        "volumes",
        "EC2",
        "RDS",
        "security",
        "groups",
        "load",
        "balancers",
        "ELBs",
        "connection",
        "stack",
        "provision",
        "machines",
        "clients",
        "AWS",
        "solutions",
        "EC2",
        "S3",
        "RDS",
        "ECS",
        "EBS",
        "Elastic",
        "Load",
        "Balancer",
        "Auto",
        "scaling",
        "groups",
        "volumes",
        "EC2",
        "instances",
        "S3",
        "buckets",
        "policies",
        "S3",
        "buckets",
        "S3",
        "bucket",
        "Glacier",
        "storage",
        "AWS",
        "AWS",
        "Cloud",
        "administration",
        "EC2",
        "instances",
        "S3",
        "SES",
        "SNS",
        "services",
        "ETL",
        "jobs",
        "web",
        "APIs",
        "REST",
        "HTTP",
        "HDFS",
        "java",
        "Talend",
        "Infrastructure",
        "team",
        "design",
        "Kafka",
        "Storm",
        "data",
        "pipeline",
        "ORM",
        "framework",
        "spring",
        "framework",
        "data",
        "persistence",
        "transaction",
        "management",
        "Environment",
        "Hadoop",
        "Hive",
        "Map",
        "Sqoop",
        "Kafka",
        "Spark",
        "Yarn",
        "Pig",
        "Cassandra",
        "Oozie",
        "shell",
        "Scripting",
        "Scala",
        "Maven",
        "Java",
        "JUnit",
        "methodologies",
        "NIFI",
        "MySQL",
        "Tableau",
        "AWS",
        "EC2",
        "S3",
        "Hortonworks",
        "power",
        "BI",
        "Solr",
        "Hadoop",
        "Developer",
        "Charter",
        "Communications",
        "St",
        "Louis",
        "MO",
        "August",
        "January",
        "Description",
        "services",
        "businesses",
        "internet",
        "access",
        "data",
        "networking",
        "phone",
        "backhaul",
        "Charter",
        "Spectrum",
        "brand",
        "name",
        "cable",
        "internet",
        "voice",
        "business",
        "services",
        "company",
        "customers",
        "US",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Hive",
        "data",
        "applications",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Reviewing",
        "Hadoop",
        "log",
        "files",
        "logs",
        "machines",
        "Developed",
        "Oozie",
        "workflow",
        "scheduling",
        "ETL",
        "process",
        "Hive",
        "Scripts",
        "apache",
        "NiFi",
        "data",
        "file",
        "system",
        "HDFS",
        "teams",
        "Anomaly",
        "detection",
        "ratings",
        "data",
        "custom",
        "input",
        "format",
        "record",
        "reader",
        "XML",
        "input",
        "SAX",
        "parser",
        "queries",
        "SparkSQL",
        "Scala",
        "Worked",
        "SPLUNK",
        "data",
        "Analyze",
        "database",
        "opensource",
        "NoSQL",
        "requirement",
        "Integrated",
        "Cassandra",
        "metadata",
        "store",
        "metadata",
        "resolution",
        "network",
        "entities",
        "network",
        "Spark",
        "Scala",
        "SparkSQL",
        "testing",
        "processing",
        "data",
        "experience",
        "RDD",
        "architecture",
        "Spark",
        "operations",
        "RDD",
        "transformations",
        "actions",
        "Spark",
        "Impala",
        "data",
        "retrieval",
        "process",
        "data",
        "Impala",
        "Tableau",
        "reporting",
        "tool",
        "dashboards",
        "connection",
        "Python",
        "packages",
        "ETL",
        "process",
        "TB",
        "data",
        "Oracle",
        "database",
        "PostgreSQL",
        "cluster",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "data",
        "Linux",
        "file",
        "system",
        "HDFS",
        "viceversa",
        "Developed",
        "UDFs",
        "DataFramesSQL",
        "RDD",
        "Spark",
        "data",
        "Aggregation",
        "queries",
        "OLTP",
        "Sqoop",
        "POC",
        "member",
        "search",
        "Solr",
        "Worked",
        "ETL",
        "methods",
        "data",
        "extraction",
        "transformation",
        "loading",
        "ETL",
        "Solutions",
        "Data",
        "warehouse",
        "tools",
        "reporting",
        "data",
        "analysis",
        "CSVExcelStorage",
        "delimiters",
        "PIG",
        "Installed",
        "Hadoop",
        "tools",
        "operating",
        "systems",
        "Ubuntu",
        "CentOS",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Modified",
        "reports",
        "Talend",
        "ETL",
        "jobs",
        "feedback",
        "QA",
        "testers",
        "Users",
        "development",
        "staging",
        "environments",
        "QA",
        "environment",
        "pig",
        "Sqoop",
        "scripts",
        "Got",
        "chance",
        "Apache",
        "NiFi",
        "Spark",
        "script",
        "Sqoop",
        "scripts",
        "NiFi",
        "scatter",
        "pattern",
        "NiFi",
        "data",
        "Postgres",
        "HDFS",
        "Fetching",
        "Hive",
        "metadata",
        "HDFS",
        "custom",
        "NiFi",
        "processor",
        "text",
        "files",
        "ETL",
        "process",
        "Talend",
        "data",
        "Worked",
        "Sqoop",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "systemsmainframe",
        "viceversa",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "operations",
        "enterprise",
        "data",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "MapReduce",
        "jobs",
        "datasets",
        "loading",
        "data",
        "Linux",
        "file",
        "systems",
        "servers",
        "java",
        "web",
        "services",
        "Kafka",
        "producers",
        "consumers",
        "code",
        "model",
        "JSON",
        "records",
        "input",
        "sources",
        "Kafka",
        "Queues",
        "streaming",
        "data",
        "Apache",
        "ignite",
        "cache",
        "data",
        "analysis",
        "logs",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "UNIX",
        "shell",
        "scripts",
        "reports",
        "Hive",
        "data",
        "Manipulate",
        "model",
        "data",
        "forms",
        "XML",
        "MapReduce",
        "MapReduce",
        "Prepared",
        "Avro",
        "schema",
        "Hive",
        "tables",
        "Created",
        "Hive",
        "tables",
        "data",
        "tables",
        "query",
        "data",
        "HQL",
        "Installed",
        "Configured",
        "Hadoop",
        "cluster",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "POC",
        "purposes",
        "Environment",
        "Hadoop",
        "MapReduce",
        "YARN",
        "Nifi",
        "HDFS",
        "PIG",
        "Hive",
        "Flume",
        "Cassandra",
        "Eclipse",
        "Ignite",
        "Core",
        "Java",
        "Sqoop",
        "Spark",
        "Splunk",
        "Maven",
        "SparkSQl",
        "Cloudera",
        "SolrTalend",
        "Linux",
        "shell",
        "JavaHadoop",
        "Developer",
        "Grubhub",
        "Chicago",
        "IL",
        "June",
        "July",
        "Description",
        "Grubhub",
        "company",
        "diners",
        "restaurants",
        "Chicago",
        "company",
        "diners",
        "restaurant",
        "partners",
        "cities",
        "United",
        "States",
        "United",
        "Kingdom",
        "idea",
        "Integrated",
        "Data",
        "Ware",
        "House",
        "Project",
        "data",
        "sources",
        "Hadoop",
        "Data",
        "Lake",
        "transformations",
        "business",
        "requirements",
        "data",
        "systems",
        "Responsibilities",
        "data",
        "DB2",
        "HDFS",
        "Sqoop",
        "Developed",
        "MapReduce",
        "jobs",
        "Java",
        "API",
        "Java",
        "engine",
        "API",
        "calls",
        "frontend",
        "JavaScript",
        "ExtJS",
        "Java",
        "methods",
        "ExtDirect",
        "Spring",
        "AOP",
        "transaction",
        "application",
        "Java",
        "batch",
        "programs",
        "Spring",
        "Batch",
        "Data",
        "Lake",
        "architecture",
        "analytics",
        "platform",
        "batch",
        "analytics",
        "requirements",
        "Hadoop",
        "Components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "YARN",
        "Map",
        "Reduce",
        "programming",
        "MapReduce",
        "programs",
        "irregularities",
        "data",
        "Hive",
        "UDFs",
        "performance",
        "results",
        "Developed",
        "Pig",
        "Latin",
        "Scripts",
        "data",
        "log",
        "files",
        "HDFS",
        "Created",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "data",
        "analysis",
        "map",
        "data",
        "sources",
        "operations",
        "algorithms",
        "Experience",
        "Sqoop",
        "data",
        "Oracle",
        "DB",
        "HDFS",
        "HIVE",
        "CRUD",
        "operations",
        "HBase",
        "data",
        "thrift",
        "API",
        "time",
        "insights",
        "workflow",
        "Oozie",
        "schedule",
        "jobs",
        "Hadoop",
        "cluster",
        "reports",
        "basis",
        "compression",
        "codecs",
        "data",
        "HDFS",
        "Avro",
        "SerDes",
        "serialization",
        "deserialization",
        "custom",
        "UDFs",
        "date",
        "functions",
        "troubleshooting",
        "issues",
        "execution",
        "MapReduce",
        "jobs",
        "log",
        "files",
        "Agile",
        "development",
        "environment",
        "sprint",
        "cycles",
        "weeks",
        "organizing",
        "tasks",
        "scrum",
        "design",
        "meetings",
        "Pig",
        "Pig",
        "Latin",
        "scripts",
        "documentation",
        "Cloudera",
        "Hadoop",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Developed",
        "Oozie",
        "MapReduce",
        "jobs",
        "Hive",
        "Queries",
        "work",
        "data",
        "HDFS",
        "data",
        "SQOOP",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "operators",
        "data",
        "DAG",
        "streams",
        "data",
        "HDFS",
        "Assisted",
        "data",
        "databases",
        "Sqoop",
        "Develop",
        "monitoring",
        "performance",
        "metrics",
        "Hadoop",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "Environment",
        "Hadoop",
        "HDFS",
        "Hive",
        "Flume",
        "Sqoop",
        "HBase",
        "PIG",
        "Eclipse",
        "Spark",
        "SQL",
        "Ubuntu",
        "Zookeeper",
        "Maven",
        "Jenkins",
        "Java",
        "JDK",
        "Oracle10",
        "g",
        "Java",
        "Developer",
        "Avineon",
        "Inc",
        "McLean",
        "VA",
        "September",
        "May",
        "Description",
        "Avineon",
        "quality",
        "information",
        "technology",
        "IT",
        "clients",
        "government",
        "business",
        "sectors",
        "aim",
        "clients",
        "business",
        "objectives",
        "help",
        "easytouse",
        "featurerich",
        "IT",
        "Avineon",
        "IT",
        "services",
        "Software",
        "Solutions",
        "Security",
        "Engineering",
        "Cloud",
        "Computing",
        "Strategic",
        "Business",
        "Services",
        "IT",
        "Service",
        "Management",
        "Responsibilities",
        "design",
        "development",
        "deployment",
        "Application",
        "JavaJ2EE",
        "Technologies",
        "Performed",
        "Requirements",
        "gathering",
        "analysis",
        "Requirements",
        "Specifications",
        "document",
        "level",
        "systems",
        "design",
        "class",
        "diagrams",
        "sequence",
        "diagrams",
        "activity",
        "diagrams",
        "user",
        "web",
        "pages",
        "part",
        "web",
        "application",
        "web",
        "technologies",
        "HTML",
        "JavaScript",
        "Angular",
        "JS",
        "AJAX",
        "CSS",
        "appearance",
        "Integrated",
        "AEM",
        "web",
        "application",
        "AEM",
        "components",
        "JavaScript",
        "CSS",
        "HTML",
        "Programmed",
        "Oracle",
        "SQL",
        "TSQL",
        "Stored",
        "Procedures",
        "Functions",
        "Triggers",
        "Packages",
        "end",
        "processes",
        "staging",
        "tables",
        "audit",
        "tables",
        "keys",
        "Maintenance",
        "support",
        "Client",
        "problems",
        "Bug",
        "fixing",
        "Deployed",
        "application",
        "Tomcat",
        "web",
        "server",
        "Analysis",
        "specifications",
        "clients",
        "JAVABEAN",
        "components",
        "AWT",
        "SWING",
        "classes",
        "Transformations",
        "Aggregator",
        "Router",
        "Joiner",
        "Expression",
        "Lookup",
        "Update",
        "Strategy",
        "Sequence",
        "Generator",
        "Exception",
        "handling",
        "performance",
        "application",
        "Core",
        "Java",
        "Business",
        "Logic",
        "call",
        "support",
        "priority",
        "issues",
        "parser",
        "framework",
        "SAX",
        "parser",
        "XML",
        "documents",
        "stores",
        "SQL",
        "Perform",
        "testing",
        "Performance",
        "testing",
        "Integration",
        "testing",
        "Regression",
        "testing",
        "Smoke",
        "testing",
        "User",
        "Acceptance",
        "Testing",
        "UAT",
        "Environment",
        "Core",
        "Java",
        "Servlets",
        "JSP",
        "XML",
        "XSLT",
        "JavaScript",
        "Apache",
        "Oracle",
        "g",
        "Jr",
        "Java",
        "Developer",
        "Emorphosys",
        "Hyderabad",
        "Telangana",
        "July",
        "August",
        "Description",
        "business",
        "telephone",
        "application",
        "Softphone",
        "IPbased",
        "phone",
        "systems",
        "Local",
        "Area",
        "Network",
        "Wide",
        "Area",
        "Network",
        "WAN",
        "server",
        "Softphone",
        "features",
        "Call",
        "Conference",
        "Call",
        "Transfer",
        "Forward",
        "Softphone",
        "Voice",
        "IP",
        "VoIP",
        "service",
        "telephony",
        "server",
        "calls",
        "computer",
        "headset",
        "handset",
        "Avaya",
        "phone",
        "Responsibilities",
        "SharePoint",
        "Confluence",
        "projects",
        "PTO",
        "Permit",
        "Operate",
        "project",
        "documents",
        "development",
        "phase",
        "Web",
        "Application",
        "Spring",
        "MVC",
        "web",
        "view",
        "application",
        "CSS",
        "HTML",
        "JSP",
        "Java",
        "Script",
        "Control",
        "M",
        "Batch",
        "Jobs",
        "IT",
        "Service",
        "Management",
        "ITSM",
        "IT",
        "services",
        "TDD",
        "Development",
        "process",
        "project",
        "functionality",
        "classes",
        "framework",
        "Mockito",
        "code",
        "Eclipse",
        "Neon",
        "IDE",
        "software",
        "development",
        "build",
        "application",
        "development",
        "environment",
        "Maven",
        "versions",
        "source",
        "code",
        "TortoiseSVN",
        "client",
        "Agile",
        "Project",
        "Management",
        "JIRA",
        "Created",
        "Application",
        "Flow",
        "Diagram",
        "Visio",
        "time",
        "creation",
        "PTO",
        "delivery",
        "testing",
        "deployment",
        "Jenkins",
        "Environment",
        "JAVA",
        "Spring",
        "MVC",
        "Hibernate",
        "JavaScript",
        "JSON",
        "Control",
        "M",
        "JIRA",
        "ITSM",
        "Eclipse",
        "Neon",
        "Maven",
        "Jenkins",
        "Visio",
        "Mockito",
        "SVN",
        "Linux",
        "Windows",
        "Education",
        "Bachelor",
        "Technology",
        "Electronics",
        "Communication",
        "Engineering",
        "University",
        "Illinois",
        "UrbanaChampaign",
        "IL",
        "Masters",
        "Science",
        "Computer",
        "Science",
        "Governors",
        "State",
        "University",
        "Chicago",
        "IL",
        "Skills",
        "JAVA",
        "years",
        "SQL",
        "years",
        "ORACLE",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "Additional",
        "Information",
        "Operating",
        "Systems",
        "Windows",
        "Linux",
        "UNIX",
        "Ubuntu",
        "Centos",
        "Programming",
        "Scripting",
        "Languages",
        "C",
        "C",
        "Core",
        "JavaJ2EE",
        "Unix",
        "Shell",
        "Scripting",
        "Python",
        "SQL",
        "Pig",
        "Latin",
        "Hive",
        "QL",
        "Scala",
        "Hadoop",
        "Distributions",
        "ClouderaCDH4CDH5",
        "Hortonworks",
        "HDP25",
        "IDEGUI",
        "Eclipse32",
        "IntelliJ",
        "Scala",
        "IDE",
        "Build",
        "Tools",
        "Jenkins",
        "Maven",
        "ANT",
        "Database",
        "Microsoft",
        "SQL",
        "Server",
        "MS",
        "SQL",
        "Oracle",
        "g",
        "DB2",
        "MySQL",
        "MSAccess",
        "MSAccess",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Cloud",
        "Computing",
        "Tools",
        "Amazon",
        "AWSAZURE",
        "Versioning",
        "Tools",
        "JIRA",
        "CVS",
        "SVN",
        "GitHub",
        "SDLC",
        "Methodologies",
        "Agile",
        "Scrum",
        "Waterfall",
        "Model"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:25:31.149968",
    "resume_data": "Sr HadoopSpark Developer Sr HadoopSpark span lDeveloperspan Sr HadoopSpark Developer Sprint Indianapolis IN Extensive IT experience of over 9 years in Analysis Design Development Implementation Maintenance and Support with experience in developing strategic methods for deploying Big Data technologies to efficiently solve Big Data processing requirement Around 5 years of experience on BIG DATA using HADOOP framework and related technologies such as HDFS Map Reduce HIVE PIG YARN APACHE SPARK FLUME KAFKA OOZIE SQOOP ZOOKEEPER and NoSQL Databases like HBase Cassandra Worked extensively on Hadoop Gen1 and Gen2 and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Resource Manager YARN Experience in working with Amazon EMR Cloudera CDH4CDH5 and Horton Works Hadoop Distributions Capable of processing large sets of structured semistructured and unstructured data and supporting systems application architecture Extensively used Apache Sqoop for efficiently importing and exporting data from HDFS to Relational Database Systems and from RDBMS to HDFS Worked on data load from various sources ie Oracle MySQL DB2 MS SQL Server Cassandra Hadoop using Sqoop and Python Script Experience in developing data pipeline using Sqoop and Flume to extract the data from weblogs and store in HDFS Experience in managing and reviewing Hadoop Log files using FLUME and Kafka and also developed the Pig UDFs and Hive UDFs to preprocess the data for analysis Worked on Impala for Massive parallel processing of Hive queries Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Efficient in working with Hive data warehouse tool creating tables data distributing by implementing Partitioning and Bucketing strategy writing and optimizing the HiveQL queries Experience in ingestion storage querying processing and analysis of Big Data with hands on experience in Big Data including Apache Spark Spark SQL and Spark Streaming Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Worked with Spark engine to process large scale data and experience to create Spark RDD and developing Spark Streaming jobs by using RDDs and leverage SparkShell Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Hands on experience in Apache Spark jobs using Scala in test environment for faster data processing and used SparkSQL for querying I have been experienced with SPARK SREAMING API to ingest data into SPARK ENGINE from KAFKA Worked on real time data integration using Kafka Storm data pipeline Spark streaming and HBase Experienced in implementing unified data platforms using Kafka producers consumers implement preprocessing using storm topologies Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Spark Good working experience on different file formats CSV Sequence files XML JSON PARQUET TEXTFILE AVRO ORC and different compression codecs GZIP SNAPPY LZO Hands on experience with NoSQL Databases like HBase Cassandra and relational databases like Oracle DB2 SQL SERVER and MySQL Expertise in job scheduling and monitoring tools like Oozie and ZooKeeper and experience in designing Oozie workflows for cleaning data and storing into Hive tables for quick analysis Strong experience in working with ELASTIC MAPREDUCE and setting up environments on Amazon AWS EC2 instances AZURE EMR and S3 Installed and configured JENKINS FOR AUTOMATING Deployments and providing automation solution Developed build and deployment scripts using ANT and MAVEN as build tools in JENKINS to move from one environment to other environments Extensive experience in ETL Data Ingestion InStream data processing Batch Analytics and Data Persistence Strategy Worked extensively with Dimensional Modeling Data Migration Data Cleansing Data Transformation and ETL Processes features for Data Warehouse System Experience with creating the TABLEAU dashboards with relational and multidimensional databases including Oracle MySQL and HIVE gathering and manipulating data from various sources Having experience in performance tuning dashboards and TABLEAU reports Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Expertise in design and development of Web Applications involving J2EE technologies with Java Spring EJB AJAX Servlets JSP Struts Web Services XML JMS JSP UNIX shell scripts SERVLETS MS SQL SERVER SOAP and RESTful web services Extensively development experience in different IDEs like Eclipse NetBeans Experience in core Java JDBC and proficient in using Java APIs for application development Experience in Deploying web application using application servers WebLogic Apache Tomcat WebSphere and JBOSS Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Work Experience Sr HadoopSpark Developer Sprint Overland Park KS February 2017 to Present Responsibilities Hands on experience in Spark and Spark Streaming creating RDD applying operations transformations and Actions Developed Spark applications using Scala for easy Hadoop transitions Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Spark code using Scala and SparkSQL for faster processing and testing Implemented Spark sample programs in python using pyspark Analyzed the SQL scripts and designed the solution to implement using pyspark Developed pyspark code to mimic the transformations performed in the onpremise environment Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Responsible for loading Data pipelines from web servers and Teradata using Sqoop with Kafka and Spark Streaming API Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Hive Populated HDFS and HBase with huge amounts of data using Apache Kafka Used Kafka to ingest data into Spark engine Configured deployed and maintained multinode Dev and Test Kafka Clusters Managing and scheduling Spark Jobs on a Hadoop Cluster using Oozie Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Designed and implemented Incremental Imports into Hive tables and writing Hive queries to run on TEZ Experienced data pipelines using Kafka and Akka for handling large terabytes of data Written shell scripts that run multiple Hive jobs which helps to automate different Hive tables incrementally which are used to generate different reports using Tableau for the Business use Experienced in Apache Spark for implementing advanced procedures like text analytics and processing using the inmemory computing capabilities written in Scala Developed Solr web apps to query and visualize and Solr indexed data from HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Involvement in creating custom UDFs for Pig and Hive to consolidate strategies and usefulness of Python into Pig Latin and HQL HiveQL Extensively worked on Text ORC Avro and Parquet file formats and compression techniques like Snappy Gzip and Zlib Implemented Hortonworks NiFi HDP 24 and recommended solution to inject data from multiple data sources to HDFS and Hive using NiFi Developed various data loading strategies and performed various transformations for analyzing the datasets by using Hortonworks Distribution for Hadoop ecosystem Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement and used Cassandra through Java services Experience in NoSQL ColumnOriented Databases like Cassandra and its Integration with Hadoop cluster Build servers using AWS importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection and open stack to provision new machines for clients Implemented AWS solutions using EC2 S3 RDS ECS EBS Elastic Load Balancer and Auto scaling groups Optimized volumes and EC2 instances Creating S3 buckets and managing policies for S3 buckets and utilized S3 bucket and Glacier for storage and backup AWS Performed AWS Cloud administration managing EC2 instances S3 SES and SNS services Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into HDFS using java and Talend Along with the Infrastructure team involved in design and developed Kafka and Storm based data pipeline ORM framework with spring framework for data persistence and transaction management Environment Hadoop Hive Map reduce Sqoop Kafka Spark Yarn Pig Cassandra Oozie shell Scripting Scala Maven Java JUnit agile methodologies NIFI MySQL Tableau AWS EC2 S3 Hortonworks power BI Solr Hadoop Developer Charter Communications St Louis MO August 2015 to January 2017 Description Its services for businesses include internet access data networking phone and wireless backhaul Charter chose Spectrum as the brand name for the combined cable internet voice and business services The combined company serves about more than 26 million residential and commercial customers across the US Responsibilities Worked on Hadoop cluster and data querying tools Hive to store and retrieve data While developing applications involved in complete Software Development Life Cycle SDLC Reviewing and managing Hadoop log files by consolidating logs from multiple machines using flume Developed Oozie workflow for scheduling ETL process and Hive Scripts Started using apache NiFi to copy the data from local file system to HDFS Involved in teams to analyze the Anomaly detection and ratings of data Implemented custom input format and record reader to read XML input efficiently using SAX parser Involved in writing queries in SparkSQL using Scala Worked with SPLUNK to analyze and visualize data Analyze database and compare it with other opensource NoSQL databases to find which one of them better suites the current requirement Integrated Cassandra as a distributed persistent metadata store to provide metadata resolution for network entities on the network Implemented Spark using Scala and SparkSQL for faster testing and processing of data Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Involved in working with Impala for data retrieval process Exported data from Impala to Tableau reporting tool created dashboards on live connection Designed multiple Python packages that were used within a large ETL process used to load 2TB of data from an existing Oracle database into a new PostgreSQL cluster Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Loaded data from Linux file system to HDFS and viceversa Developed UDFs using both DataFramesSQL and RDD in Spark for data Aggregation queries and reverting back into OLTP through Sqoop POC for enabling member and suspect search using Solr Worked on ETL methods for data extraction transformation and loading in corporatewide ETL Solutions and Data warehouse tools for reporting and data analysis Used CSVExcelStorage to parse with different delimiters in PIG Installed and monitored Hadoop ecosystems tools on multiple operating systems like Ubuntu CentOS Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Modified reports and Talend ETL jobs based on the feedback from QA testers and Users in development and staging environments Involved in setting QA environment by implementing pig and Sqoop scripts Got chance working on Apache NiFi like executing Spark script Sqoop scripts through NiFi worked on creating scatter and gather pattern in NiFi ingesting data from Postgres to HDFS Fetching Hive metadata and storing in HDFS created a custom NiFi processor for filtering text from Flow files etc Responsible for designing and implementing ETL process using Talend to load data from Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systemsmainframe and viceversa Developed Pig Latin scripts to do operations of sorting joining and filtering enterprise data Implemented test scripts to support test driven development and integration Developed multiple MapReduce jobs in java to clean datasets Involved in loading data from Linux file systems servers java web services using Kafka producers and consumers Involved in developing code to write canonical model JSON records from numerous input sources to Kafka Queues Performed streaming of data into Apache ignite by setting up cache for efficient data analysis Collected the logs data from web servers and integrated in to HDFS using Flume Developed UNIX shell scripts for creating the reports from Hive data Manipulate serialize model data in multiple forms like JSON XML Involved in setting up MapReduce 1 and MapReduce 2 Prepared Avro schema files for generating Hive tables and Created Hive tables and loaded the data in to tables and query data using HQL Installed and Configured Hadoop cluster using Amazon Web Services AWS for POC purposes Environment Hadoop MapReduce 2 YARN Nifi HDFS PIG Hive Flume Cassandra Eclipse Ignite Core Java Sqoop Spark Splunk Maven SparkSQl Cloudera SolrTalend Linux shell scripting JavaHadoop Developer Grubhub Chicago IL June 2013 to July 2015 Description Grubhub is an online and mobile foodordering company that connects diners with local restaurants Based in Chicago the company has more than 918 million active diners and more than 55000 restaurant partners in over 1100 cities across the United States and the United Kingdom The idea of Integrated Data Ware House Project is to ingest data from different multiple sources to Hadoop Data Lake perform transformations on it according to business requirements and loading the data to external systems Responsibilities Exported data from DB2 to HDFS using Sqoop and Developed MapReduce jobs using Java API Designed and implemented Java engine and API to perform direct calls from frontend JavaScript ExtJS to serverside Java methods ExtDirect Used Spring AOP to implement Distributed declarative transaction throughout the application Designed and developed Java batch programs in Spring Batch Worked on Data Lake architecture to build a reliable scalable analytics platform to meet batch interactive and online analytics requirements Concerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented optimized map joins to get data from different sources to perform cleaning operations before applying the algorithms Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Used various compression codecs to effectively compress the data in HDFS Used Avro SerDes for serialization and deserialization and also implemented hive custom UDFs involving date functions Responsible for troubleshooting issues in the execution of MapReduce jobs by inspecting and reviewing log files Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Installed and configured Pig and wrote Pig Latin scripts Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Developed workflowusing Oozie for running MapReduce jobs and Hive Queries Done the work in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP Involved in loading data from UNIX file system to HDFS Created java operators to process data using DAG streams and load data to HDFS Assisted in exporting analyzed data to relational databases using Sqoop Involved in Develop monitoring and performance metrics for Hadoop clusters Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment Hadoop HDFS Hive Flume Sqoop HBase PIG Eclipse Spark My SQL and Ubuntu Zookeeper Maven Jenkins Java JDK 16 Oracle10g Java Developer Avineon Inc McLean VA September 2011 to May 2013 Description Avineon has been providing high quality information technology IT solutions to clients operating in both government and business sectors Its aim is to meet our clients business and organizational objectives with the help of easytouse featurerich IT solutions Avineon offers IT services in Software Solutions Security Engineering Cloud Computing and Strategic Business Services and IT Service Management Responsibilities Involved in the design development and deployment of the Application using JavaJ2EE Technologies Performed Requirements gathering and analysis and prepared Requirements Specifications document Provided high level systems design specifying the class diagrams sequence diagrams and activity diagrams Involved in designing user interactive web pages as the frontend part of the web application using various web technologies like HTML JavaScript Angular JS AJAX and implemented CSS for better appearance and feel Integrated AEM to the existing web application and created AEM components using JavaScript CSS and HTML Programmed Oracle SQL TSQL Stored Procedures Functions Triggers and Packages as back end processes to create and update staging tables log and audit tables and creating primary keys Provided further Maintenance and support this involves working with the Client and solving their problems which include major Bug fixing Deployed and tested the application using Tomcat web server Analysis of the specifications provided by the clients Developed JAVABEAN components utilizing AWT and SWING classes Extensively used Transformations like Aggregator Router Joiner Expression Lookup Update Strategy and Sequence Generator Used Exception handling and Multithreading for the optimum performance of the application Used the Core Java concepts to implement the Business Logic Provided on call support based on the priority of the issues Designed and implemented a generic parser framework using SAX parser to parse XML documents which stores SQL Perform Functional testing Performance testing Integration testing Regression testing Smoke testing and User Acceptance Testing UAT Environment Core Java Servlets struts JSP XML XSLT JavaScript Apache Oracle 10g11g Jr Java Developer Emorphosys Hyderabad Telangana July 2009 to August 2011 Description This is a business telephone application known as Softphone which is designed to work with IPbased phone systems and Local Area Network or Wide Area Network WAN The server supports the Softphone features like Call Conference Call Transfer and Call Forward The Softphone provides Voice over IP VoIP service using a telephony server It is used to make and take calls through computer using a headset or handset connected to Avaya Hard phone Responsibilities Documented in SharePoint and Confluence of projects Created the PTO Permit To Operate of the project to provide the supported documents before the development phase Developed the Web Application using Spring MVC To develop the web view of the application used CSS HTML JSP and Java Script Used Control M to configure and schedule the Batch Jobs IT Service Management ITSM is used to plan deliver operate and control IT services offered In the TDD Development process of the project created and tested the functionality of the classes with mocking framework Mockito For writing the code used Eclipse Neon IDE for the software development For build and deploy the application to development environment used Maven Managed the different versions of the source code with TortoiseSVN client For Agile Project Management used JIRA Created Application Flow Diagram using Visio at the time creation of PTO For continuous delivery testing and deployment used Jenkins Environment JAVA 18 Spring MVC 32 Hibernate 42 JavaScript JSON 22 Control M JIRA ITSM Eclipse Neon Maven 339 Jenkins Visio Mockito 22 SVN Linux Windows 7 Education Bachelor of Technology in Electronics and Communication Engineering University of Illinois UrbanaChampaign IL Masters of Science in Computer Science Governors State University Chicago IL Skills JAVA 9 years SQL 6 years ORACLE 5 years Hadoop 5 years HADOOP 5 years Additional Information Operating Systems Windows Linux UNIX Ubuntu Centos Programming or Scripting Languages C C Core JavaJ2EE Unix Shell Scripting Python SQL Pig Latin Hive QL Scala Hadoop Distributions ClouderaCDH4CDH5 Hortonworks HDP25 IDEGUI Eclipse32 IntelliJ Scala IDE Build Tools Jenkins Maven ANT Database Microsoft SQL Server MS SQL Oracle 11g10g DB2 MySQL MSAccess MSAccess NoSQL HBase Cassandra Cloud Computing Tools Amazon AWSAZURE Versioning Tools JIRA CVS SVN and GitHub SDLC Methodologies Agile Scrum Waterfall Model",
    "unique_id": "effc0a85-08d4-4ee5-8e5a-f44d96680ae0"
}